# -*- coding: utf-8 -*-
"""Zeru Round 2 Wallet Risk Scoring Assignment .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dq8zAnVTa1Vmd_wa-fO773Uno5eLTM11

Step 1: Importing files and necessary libraries
"""

!pip install pandas requests

import pandas as pd
import requests

print("Pandas version:", pd.__version__)
print("Requests version:", requests.__version__)

!pip install pandas openpyxl requests tqdm

from google.colab import files
import pandas as pd

# Here we need to upload file manually
uploaded = files.upload()

wallets_df = pd.read_excel(next(iter(uploaded)), engine='openpyxl')
wallets_df.head()

# Display first 5 wallet addresses
print("Total wallets:", len(wallets_df))
wallets_df.head()

"""Step 2: Data Collection"""

import requests
import json
from tqdm import tqdm

"""Transaction Fetching"""

import requests

# Your API key and subgraph URL
API_KEY = "18738c8d699951515b498b22406769d3"
SUBGRAPH_URL = "https://gateway.thegraph.com/api/subgraphs/id/4TbqVA8p2DoBd5qDbPMwmDZv3CsJjWtxo8nVSqF2tA9a"

# Function to fetch borrow events for a wallet
def fetch_borrows(wallet_address):
    query = """
    {
      borrows(first: 1000, where: {account: "%s"}) {
        id
        amount
        timestamp
        account {
          id
        }
      }
    }
    """ % wallet_address.lower()

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

    response = requests.post(SUBGRAPH_URL, json={"query": query}, headers=headers)

    if response.status_code == 200:
        return response.json().get("data", {}).get("borrows", [])
    else:
        print(f"Error {response.status_code}: {response.text}")
        return []

test_wallet = "0x0039f22efb07a647557c7c5d17854cfd6d489ef3"
borrow_data = fetch_borrows(test_wallet)

# Display results
print(f"Borrow events for {test_wallet}:")
for b in borrow_data:
    print(b)

# Commented out IPython magic to ensure Python compatibility.
def fetch_events(wallet_address, event_type):
    query_template = """
    {
#       %s(first: 1000, where: {account: "%s"}) {
        id
        amount
        timestamp
        account {
          id
        }
      }
    }
    """ % (event_type, wallet_address.lower())

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

    response = requests.post(SUBGRAPH_URL, json={"query": query_template}, headers=headers)

    if response.status_code == 200:
        return response.json().get("data", {}).get(event_type, [])
    else:
        print(f"Error {response.status_code} for {event_type} in {wallet_address}")
        return []

from tqdm import tqdm

all_wallet_data = []

for wallet in tqdm(wallets_df['wallet_id']):
    wallet_record = {
        "wallet_id": wallet,
        "borrows": fetch_events(wallet, "borrows"),
        "repays": fetch_events(wallet, "repays"),
        #"liquidations": fetch_events(wallet, "liquidations"),
        #"mints": fetch_events(wallet, "mints"),
        #"redeems": fetch_events(wallet, "redeems"),
    }
    all_wallet_data.append(wallet_record)

import json

with open("raw_wallet_data.json", "w") as f:
    json.dump(all_wallet_data, f)

import pandas as pd

borrow_records = []
repay_records = []

for record in all_wallet_data:
    wallet = record["wallet_id"]

    for borrow in record.get("borrows", []):
        borrow_records.append({
            "wallet_id": wallet,
            "type": "borrow",
            "amount": int(borrow["amount"]),
            "timestamp": int(borrow["timestamp"])
        })

    for repay in record.get("repays", []):
        repay_records.append({
            "wallet_id": wallet,
            "type": "repay",
            "amount": int(repay["amount"]),
            "timestamp": int(repay["timestamp"])
        })

df = pd.DataFrame(borrow_records + repay_records)
df["datetime"] = pd.to_datetime(df["timestamp"], unit="s")
df.head()

# Group by wallet and type to calculate total amount
summary = df.groupby(["wallet_id", "type"])["amount"].sum().unstack(fill_value=0)

# Here we're Computing net debt also
summary["net_borrowed"] = summary.get("borrow", 0) - summary.get("repay", 0)
summary = summary.sort_values("net_borrowed", ascending=False)
summary.head()

"""Step 3: Data Visualization"""

import matplotlib.pyplot as plt

# Unique wallets per type
wallet_counts = df.groupby("type")["wallet_id"].nunique()
print(wallet_counts)

import matplotlib.pyplot as plt

# Filter data
borrow_df = df[df['type'] == 'borrow']
repay_df = df[df['type'] == 'repay']

# Group by date
borrow_ts = borrow_df.groupby('datetime')['amount'].sum()
repay_ts = repay_df.groupby('datetime')['amount'].sum()

# Plot
plt.figure(figsize=(14, 6))
borrow_ts.plot(label='Borrow Volume', color='red')
repay_ts.plot(label='Repay Volume', color='green')
plt.title("Borrow vs Repay Volume Over Time")
plt.xlabel("Date")
plt.ylabel("Total Volume")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Aggregate borrow and repay amounts per wallet
borrow_per_wallet = borrow_df.groupby('wallet_id')['amount'].sum()
repay_per_wallet = repay_df.groupby('wallet_id')['amount'].sum()

# Combine into one DataFrame
wallet_summary = pd.DataFrame({
    'Total Borrowed': borrow_per_wallet,
    'Total Repaid': repay_per_wallet
}).fillna(0)

# Plot top 20 wallets
wallet_summary.head(20).plot(kind='bar', figsize=(15, 6), title='Top 20 Wallets: Borrow vs Repay')
plt.ylabel("Amount")
plt.xlabel("Wallet")
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Calculate risk as borrow - repay
wallet_summary['Net Risk'] = wallet_summary['Total Borrowed'] - wallet_summary['Total Repaid']

# Sort by highest risk
risky_wallets = wallet_summary.sort_values(by='Net Risk', ascending=False).head(10)

# Plot risky wallets
risky_wallets[['Total Borrowed', 'Total Repaid']].plot(kind='bar', figsize=(14, 6), title="Top 10 Risky Wallets")
plt.ylabel("Amount")
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()



"""Step 4: Data Preparation & Feature Engineering"""



import pandas as pd

df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')

# Group by wallet
wallet_groups = df.groupby('wallet_id')

# Create feature dataframe
feature_df = wallet_groups.agg(
    total_borrowed=('amount', lambda x: x[df.loc[x.index, 'type'] == 'borrow'].sum()),
    total_repaid=('amount', lambda x: x[df.loc[x.index, 'type'] == 'repay'].sum()),
    borrow_count=('type', lambda x: (x == 'borrow').sum()),
    repay_count=('type', lambda x: (x == 'repay').sum()),
    avg_borrow_amount=('amount', lambda x: x[df.loc[x.index, 'type'] == 'borrow'].mean()),
    avg_repay_amount=('amount', lambda x: x[df.loc[x.index, 'type'] == 'repay'].mean()),
    first_activity_date=('timestamp', 'min'),
    last_activity_date=('timestamp', 'max')
).fillna(0)

# Derived features
feature_df['active_days'] = (feature_df['last_activity_date'] - feature_df['first_activity_date']).dt.days + 1
feature_df['borrow_frequency'] = feature_df['borrow_count'] / feature_df['active_days']
feature_df['repay_frequency'] = feature_df['repay_count'] / feature_df['active_days']
feature_df['net_balance'] = feature_df['total_borrowed'] - feature_df['total_repaid']
feature_df['repay_to_borrow_ratio'] = feature_df['total_repaid'] / feature_df['total_borrowed'].replace(0, 1)
feature_df['is_risky'] = ((feature_df['repay_to_borrow_ratio'] < 0.5) & (feature_df['net_balance'] > 0)).astype(int)

# Reset index for further use
feature_df = feature_df.reset_index()

# Show final engineered features
feature_df.head()

#Here I am again writhing code for converting few columns from exponential to human readable format

import pandas as pd

# Convert timestamp to datetime
df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')

# Group by wallet
wallet_groups = df.groupby('wallet_id')

# Create feature dataframe
feature_df = wallet_groups.agg(
    total_borrowed=('amount', lambda x: x[df.loc[x.index, 'type'] == 'borrow'].sum()),
    total_repaid=('amount', lambda x: x[df.loc[x.index, 'type'] == 'repay'].sum()),
    borrow_count=('type', lambda x: (x == 'borrow').sum()),
    repay_count=('type', lambda x: (x == 'repay').sum()),
    avg_borrow_amount=('amount', lambda x: x[df.loc[x.index, 'type'] == 'borrow'].mean()),
    avg_repay_amount=('amount', lambda x: x[df.loc[x.index, 'type'] == 'repay'].mean()),
    first_activity_date=('timestamp', 'min'),
    last_activity_date=('timestamp', 'max')
).fillna(0)

# Derived features
feature_df['active_days'] = (feature_df['last_activity_date'] - feature_df['first_activity_date']).dt.days + 1
feature_df['borrow_frequency'] = feature_df['borrow_count'] / feature_df['active_days']
feature_df['repay_frequency'] = feature_df['repay_count'] / feature_df['active_days']
feature_df['net_balance'] = feature_df['total_borrowed'] - feature_df['total_repaid']
feature_df['repay_to_borrow_ratio'] = feature_df['total_repaid'] / feature_df['total_borrowed'].replace(0, 1)
feature_df['is_risky'] = ((feature_df['repay_to_borrow_ratio'] < 0.5) & (feature_df['net_balance'] > 0)).astype(int)

# Function to convert large numbers into human-readable format
def readable_format(x):
    if abs(x) >= 1e9:
        return f'{x/1e9:.2f}B'
    elif abs(x) >= 1e6:
        return f'{x/1e6:.2f}M'
    elif abs(x) >= 1e3:
        return f'{x/1e3:.2f}K'
    else:
        return f'{x:.2f}'

# Apply formatting to relevant columns for visualization
for col in ['total_borrowed', 'total_repaid', 'net_balance', 'avg_borrow_amount', 'avg_repay_amount']:
    feature_df[f'{col}_readable'] = feature_df[col].apply(readable_format)

# Reset index for final DataFrame
feature_df = feature_df.reset_index()

# Show engineered features with readable values
feature_df.head()



"""Step 5: Feature Normalization & Building Risk Scoring Model"""

# weights = {
#     'repay_to_borrow_ratio': -0.5,  # higher ratio = safer → negative weight
#     'net_balance': 0.3,             # higher debt = higher risk
#     'borrow_frequency': 0.2,        # frequent borrowing = slightly riskier
#     'is_risky': 0.5                 # strong signal
# }

# from sklearn.preprocessing import MinMaxScaler
# import numpy as np

# # Copy the feature_df to avoid overwriting
# scoring_df = feature_df.copy()

# # Select features for scoring
# scoring_features = ['repay_to_borrow_ratio', 'net_balance', 'borrow_frequency', 'is_risky']

# # Normalize selected features using MinMaxScaler
# scaler = MinMaxScaler()
# scoring_df_norm = pd.DataFrame(scaler.fit_transform(scoring_df[scoring_features]),
#                                columns=[f"{col}_norm" for col in scoring_features])

# # Add normalized columns to original dataframe
# for col in scoring_df_norm.columns:
#     scoring_df[col] = scoring_df_norm[col]



# Assign weights to each normalized feature
weights = {
    'repay_to_borrow_ratio_norm': -0.5,   # lower ratio = riskier
    'net_balance_norm': 0.3,              # more unpaid = riskier
    'borrow_frequency_norm': 0.2,         # frequent = slightly riskier
    'is_risky_norm': 0.5                  # boolean flag
}

# Compute weighted score
scoring_df['raw_score'] = (
    weights['repay_to_borrow_ratio_norm'] * scoring_df['repay_to_borrow_ratio_norm'] +
    weights['net_balance_norm'] * scoring_df['net_balance_norm'] +
    weights['borrow_frequency_norm'] * scoring_df['borrow_frequency_norm'] +
    weights['is_risky_norm'] * scoring_df['is_risky']
)

# Normalize raw_score to 0–1000 range
min_score = scoring_df['raw_score'].min()
max_score = scoring_df['raw_score'].max()

scoring_df['risk_score'] = ((scoring_df['raw_score'] - min_score) / (max_score - min_score)) * 1000
scoring_df['risk_score'] = scoring_df['risk_score'].round(2)

# Final output
final_wallet_risk = scoring_df[['wallet_id', 'risk_score']].sort_values(by='risk_score', ascending=False)
final_wallet_risk.head()



from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd

# Copy the feature_df to avoid overwriting
scoring_df = feature_df.copy()

# Select features for scoring
scoring_features = ['repay_to_borrow_ratio', 'net_balance', 'borrow_frequency', 'is_risky']

# Normalize selected features using MinMaxScaler
scaler = MinMaxScaler()
scoring_df_norm = pd.DataFrame(scaler.fit_transform(scoring_df[scoring_features]),
                               columns=[f"{col}_norm" for col in scoring_features])

# Add normalized columns to scoring_df
for col in scoring_df_norm.columns:
    scoring_df[col] = scoring_df_norm[col]

# Assign weights to each normalized feature
weights = {
    'repay_to_borrow_ratio_norm': -0.5,   # lower = riskier
    'net_balance_norm': 0.3,
    'borrow_frequency_norm': 0.2,
    'is_risky_norm': 0.5
}

# Compute weighted raw score
scoring_df['raw_score'] = (
    weights['repay_to_borrow_ratio_norm'] * scoring_df['repay_to_borrow_ratio_norm'] +
    weights['net_balance_norm'] * scoring_df['net_balance_norm'] +
    weights['borrow_frequency_norm'] * scoring_df['borrow_frequency_norm'] +
    weights['is_risky_norm'] * scoring_df['is_risky']  # is_risky already 0/1, no need to normalize again
)

# Normalize raw_score to range 0–1000
min_score = scoring_df['raw_score'].min()
max_score = scoring_df['raw_score'].max()
scoring_df['risk_score'] = ((scoring_df['raw_score'] - min_score) / (max_score - min_score)) * 1000
scoring_df['risk_score'] = scoring_df['risk_score'].round(2)

# Display all 103 wallets sorted by risk
final_wallet_risk = scoring_df[['wallet_id', 'risk_score']].sort_values(by='risk_score', ascending=False)

# Show all rows
pd.set_option('display.max_rows', None)
display(final_wallet_risk)

import pandas as pd


scoring_df.to_csv("wallet_risk_scores.csv", index=False)













